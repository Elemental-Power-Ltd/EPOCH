"""
Carbon Itensity endpoints, including generation and getting.

Carbon intensity is a measure of kg CO2 e / kWh for electrical power used, and
varies over time as the grid changes.
"""

import datetime
import itertools
import logging
import operator
import typing
from typing import cast

import aiometer
import numpy as np
import pandas as pd
import pydantic
from app.internal.epl_typing import RecordMapping
from app.internal.utils import split_into_sessions
from fastapi import APIRouter, HTTPException

from ..dependencies import DatabasePoolDep, HTTPClient, HttpClientDep
from ..internal.client_data import get_postcode
from ..internal.site_manager.bundles import file_self_with_bundle
from ..internal.utils import chunk_time_period
from ..internal.utils.uuid import uuid7
from ..models.carbon_intensity import CarbonIntensityMetadata, EpochCarbonEntry, GridCO2Request
from ..models.core import DatasetIDWithTime, DatasetTypeEnum
from .import_tariffs import get_gsp_code_from_postcode

logger = logging.getLogger(__name__)
router = APIRouter()


class CarbonIntensityRawEntry(typing.TypedDict):
    """Data we get back from the CarbonIntensity API, including fuel types."""

    start_ts: pydantic.AwareDatetime
    end_ts: pydantic.AwareDatetime
    forecast: typing.NotRequired[float]
    actual: typing.NotRequired[float]
    biomass: typing.NotRequired[float]
    coal: typing.NotRequired[float]
    imports: typing.NotRequired[float]
    gas: typing.NotRequired[float]
    hydro: typing.NotRequired[float]
    nuclear: typing.NotRequired[float]
    other: typing.NotRequired[float]
    solar: typing.NotRequired[float]
    wind: typing.NotRequired[float]


def interpolate_carbon_intensity(
    new_times: pd.DatetimeIndex, raw_data: list[CarbonIntensityRawEntry]
) -> list[CarbonIntensityRawEntry]:
    """
    Interpolate a carbon intensity response to fill in the gaps.

    Often, the 3rd party API will have missing sections of data, either due to small bugs or totally missing readings.
    In those cases, we have to re-interpolate ourselves to fill those in, which isn't strictly accurate but
    is better than nothing (the fuel type interpolation will be especially inaccurate, as it may predict solar overnight).

    There are many different fuel types we'll have to interpolate over, which may and or may not be provided from the 3rd party.

    Parameters
    ----------
    new_times
        Series of datetimes to interpolate to, probably generated by pd.date_range.
        These will become start_ts values, and the end_ts values will be +30 minutes from these.

    raw_data
        List of raw data we got from the carbon intensity API, ideally with fuel types but maybe with chunks missing.

    Returns
    -------
        Data of same structure as raw_data, but interpolated linearly at the new_times
    """
    # Only the start_ts and end_ts are required keys, as the fuels and actual / forecast may not be present.
    keys = list(CarbonIntensityRawEntry.__optional_keys__)
    interpolated_data = {}

    # The x coordinates are differences from the specified start time, which can be negative
    # for numerical stability reasons (it's much easier to interpolate around 0 than it is to in
    # interpolate around 1e9).
    start_ts = min(new_times)
    xs = (new_times - start_ts).total_seconds().to_numpy()
    for key in keys:
        # We re-do the xs each time as we have to check if the relevant key is in this entry,
        # which is might not be.
        xp = np.asarray([
            (item["start_ts"] - start_ts).total_seconds()
            for item in raw_data
            if item.get(key) is not None and np.isfinite(item[key])  # type: ignore
        ])
        yp = np.asarray([
            item[key]  # type: ignore
            for item in raw_data
            if item.get(key) is not None and np.isfinite(item[key])  # type: ignore
        ])
        interpolated_data[key] = np.full_like(xp, np.nan) if len(xp) == 0 or len(yp) == 0 else np.interp(xs, xp, yp)

    return [
        {
            "start_ts": time,
            "end_ts": time + pd.Timedelta(minutes=30),
            **{key: interpolated_data[key][i] for key in keys if len(interpolated_data[key]) > i},  # type: ignore
        }
        for i, time in enumerate(new_times)
    ]


async def fetch_carbon_intensity_batch(
    client: HTTPClient,
    postcode: str | None,
    timestamps: tuple[pydantic.AwareDatetime, pydantic.AwareDatetime],
) -> list[CarbonIntensityRawEntry]:
    """
    Fetch a single lot of data from the carbon itensity API.

    This is in its own (rather ugly) function to allow the rate limiter to function.
    Will return a `forecast` carbon itensity and a breakdown if using regional, or an actual if not.

    Parameters
    ----------
    client
        HTTPX AsyncClient that we re-use between connections

    postcode
        The entire postcode for this site with inbound and outbound codes split by a space (e.g. `SW1 1AA`).
        If None, we'll retrieve national data instead of regional.

    timestamps
        A (start_ts, end_ts) pairing for the time period we want to check. Should be within 14 days of each other.
        If these are the same, we'll push the end timestamp to be +24 hours, so watch out for getting extra data.

    Returns
    -------
    results
        A list of carbon intensity readings and their times. There is no guarantee that there are in order,
        and they may request data outside of the specified timestamp window.
    """
    fetch_start_ts, fetch_end_ts = timestamps
    if isinstance(fetch_start_ts, pd.Timestamp):
        fetch_start_ts = fetch_start_ts.to_pydatetime()
    if isinstance(fetch_end_ts, pd.Timestamp):
        fetch_end_ts = fetch_end_ts.to_pydatetime()

    # If we got the start and end the same, then this is probably trying to work around a bug
    # in the CarbonIntensity API which will stop at 23:30 sometimes.
    # In that case, fetch an extra day and let the next function sort it out.
    if fetch_start_ts == fetch_end_ts:
        fetch_end_ts = fetch_start_ts + pd.Timedelta(days=1)
    start_ts_str = fetch_start_ts.isoformat()
    end_ts_str = fetch_end_ts.isoformat()

    if postcode is not None:
        postcode_out = postcode.strip().split(" ")[0]
        ci_url = f"https://api.carbonintensity.org.uk/regional/intensity/{start_ts_str}/{end_ts_str}/postcode/{postcode_out}"
    else:
        # Retrieve national data instead of regional, as we didn't get a postcode.
        ci_url = f"https://api.carbonintensity.org.uk/intensity/{start_ts_str}/{end_ts_str}"

    response = await client.get(ci_url)
    if not response.status_code == 200:
        raise HTTPException(400, f"{ci_url} returned `{response.text}`")

    data = response.json()
    results: list[CarbonIntensityRawEntry] = []
    subdata = data["data"]
    if "data" in subdata:
        # Sometimes we get a nested object one deep, especially for regional data
        subdata = subdata["data"]

    for item in subdata:
        entry: CarbonIntensityRawEntry = {
            "start_ts": pd.to_datetime(item["from"]),
            "end_ts": pd.to_datetime(item["to"]),
            "forecast": item["intensity"].get("forecast", float("NaN")),
            "actual": item["intensity"].get("actual", float("NaN")),
        }
        for fuel_data in item.get("generationmix", []):
            entry[fuel_data["fuel"]] = fuel_data["perc"] / 100.0  # type: ignore
        results.append(entry)
    return results


async def fetch_carbon_intensity_remote(
    client: HTTPClient,
    postcode: str | None,
    start_ts: pydantic.AwareDatetime,
    end_ts: pydantic.AwareDatetime,
) -> list[CarbonIntensityRawEntry]:
    """
    Fetch data from the carbon intensity API, doing the cleaning and interpolation as necessary.

    Will return a `forecast` carbon itensity and a breakdown if using regional, or an actual if not.
    Individual fuels are interpolated where possible, which may not be accurate. There's no guarantee that
    the fuel breakdown is correct, or provided by the 3rd party.

    Parameters
    ----------
    client
        HTTPX AsyncClient to make requests to the CarbonIntensity API with.

    postcode
        The entire postcode for this site with inbound and outbound codes split by a space (e.g. `SW1 1AA`).
        If provided, we'll request data for this region. If not, we'll request national data.

    start_ts
        The earliest timestamp to request data for such that min(data["start_ts"]) == start_ts

    end_ts
        The latest timestamp to request data for, such that max(data["end_ts"]) == end_ts
        We'll interpolate between these times if there's data missing on the service, and chunk the requests
        to be in ~14 day periods and split across years (to avoid a bug in the 3rd party)

    Returns
    -------
    results
        A list of carbon intensity readings and their times. These have been sorted and interpolated between
        the times.
    """
    # There's a bug in the CarbonIntensity API that doesn't like non-zero seconds, especially not closely spaced ones.
    # Instead, let's just grab the period 1 day either side and sort it out in the interpolation.
    rounded_start_ts = pd.Timestamp(start_ts).floor("1h").to_pydatetime()
    rounded_end_ts = pd.Timestamp(end_ts).ceil("1h").to_pydatetime()
    # There's another bug in the CarbonIntensity API that doesn't wrap around years. Instead,
    # we have to manually split the years here.
    time_pairs = chunk_time_period(rounded_start_ts, rounded_end_ts, pd.Timedelta(days=13), split_years=True)
    all_data: list[CarbonIntensityRawEntry] = []
    async with aiometer.amap(
        lambda ts_pair: fetch_carbon_intensity_batch(client=client, postcode=postcode, timestamps=ts_pair),
        time_pairs,
        # This is a horrible bodge, but for testing we have a mocked client
        # where we want to do
        max_at_once=1 if getattr(client, "DO_RATE_LIMIT", True) else None,
        max_per_second=1 if getattr(client, "DO_RATE_LIMIT", True) else None,
    ) as results:
        async for result in results:
            all_data.extend(result)

    if not all_data:
        raise HTTPException(400, "Failed to get grid CO2 data.")

    # Now we've got the data, we should tidy it up.
    # This involves sorting, filter and interpolate it
    all_data = sorted(all_data, key=operator.itemgetter("start_ts"))
    new_times = pd.date_range(start_ts, end_ts, freq=pd.Timedelta(minutes=30), inclusive="left")
    return interpolate_carbon_intensity(new_times, all_data)


async def postcode_to_db_gsp(postcode: str | None, http_client: HttpClientDep) -> str:
    """
    Turn a postcode into a GSP that is suitable for the database.

    If the postcode is None, then we'll return the string "uk" to map to the national carbon intensity table.
    Otherwise, we'll return the single-letter GSP code for your region.

    Parameters
    ----------
    postcode
        Postcode with a space in the middle between incoming and outgoing to look up.
    http_client
        HTTP client used to request from the carbon intensity API

    Returns
    -------
    str
        single character if you provided a postcode e.g. "A" for Eastern England.
    "uk"
        If postcode is None
    """
    if postcode is None:
        return "uk"
    # Use the letters as they're a bit saner than the various IDs that are used.
    gsp_code = await get_gsp_code_from_postcode(postcode.split(" ")[0], http_client=http_client)
    # Octopus uses codes prefixed with "_" which is very annoying.
    return gsp_code.region_code.value.replace("_", "")


async def fetch_carbon_intensity(
    pool: DatabasePoolDep,
    postcode: str | None,
    start_ts: pydantic.AwareDatetime,
    end_ts: pydantic.AwareDatetime,
    http_client: HttpClientDep,
) -> list[CarbonIntensityRawEntry]:
    """
    Fetch data from the carbon intensity API, doing the cleaning and interpolation as necessary.

    Will return a `forecast` carbon itensity and a breakdown if using regional, or an actual if not.
    Individual fuels are interpolated where possible, which may not be accurate. There's no guarantee that
    the fuel breakdown is correct, or provided by the 3rd party.

    Parameters
    ----------
    client
        HTTPX AsyncClient to make requests to the CarbonIntensity API with.

    postcode
        The entire postcode for this site with inbound and outbound codes split by a space (e.g. `SW1 1AA`).
        If provided, we'll request data for this region. If not, we'll request national data.

    start_ts
        The earliest timestamp to request data for such that min(data["start_ts"]) == start_ts

    end_ts
        The latest timestamp to request data for, such that max(data["end_ts"]) == end_ts
        We'll interpolate between these times if there's data missing on the service, and chunk the requests
        to be in ~14 day periods and split across years (to avoid a bug in the 3rd party)

    pool
        Database pool to cache into

    Returns
    -------
    results
        A list of carbon intensity readings and their times.
        These have been sorted and interpolated between the times.
        Only returns the fields `actual`, and `forecast` currently, although the rest are cached.
    """
    gsp_code = await postcode_to_db_gsp(postcode, http_client=http_client)
    rows = await pool.fetch(
        """
        SELECT
            start_ts,
            end_ts,
            actual,
            forecast
        FROM carbon_intensity.grid_co2_cache
        WHERE
            start_ts >= $1 AND end_ts <= $2 AND gsp_code = $3
        ORDER BY start_ts ASC""",
        start_ts,
        end_ts,
        gsp_code,
    )

    # This slight offset to the end date is to make sure that we don't try to get data for the whole day
    # on end_ts, which should be midnight at the next day.
    expected_days = {
        item.date() for item in pd.date_range(start_ts, end_ts - pd.Timedelta(seconds=1), freq=pd.Timedelta(days=1))
    }
    found_days = {item["start_ts"].date() for item in rows}
    missing_days = sorted(expected_days - found_days)
    if not missing_days:
        # We've got everything! This is the happy path, and we're all good from here.
        return [
            CarbonIntensityRawEntry(
                start_ts=item["start_ts"], end_ts=item["end_ts"], forecast=item["forecast"], actual=item["actual"]
            )
            for item in rows
        ]

    # otherwise we're in trouble and have to get the missing days.
    # To do this, we try to make the minimum number of requests to the third party API as it's really slow.
    # So split the missing days into chunks, e.g. 1, 2, 3, 10, 11, 12 will split into
    # [[1, 2, 3], [10, 11, 12]]
    # Then we group that into [(1, 3), (10, 12)]
    # If any of those groups are too long, then chunk them up e.g.
    # [(100, 120)] will be chunked into [(100, 113), (113, 120)]
    MAX_PERIOD = pd.Timedelta(days=13)
    day_sessions = split_into_sessions(
        [pd.Timestamp(item, tz=datetime.UTC) for item in missing_days], max_diff=pd.Timedelta(days=1)
    )
    chunked_sessions = []
    for session in day_sessions:
        chunked_sessions.extend(chunk_time_period(min(session), max(session), freq=MAX_PERIOD, split_years=True))

    # Get the data from the third party in batches, then re-assemble and sort it.
    all_data: list[CarbonIntensityRawEntry] = []
    async with aiometer.amap(
        lambda ts_pair: fetch_carbon_intensity_batch(client=http_client, postcode=postcode, timestamps=ts_pair),
        chunked_sessions,
        # This is a horrible bodge, but for testing we have a mocked client
        # where we want to do
        max_at_once=1 if getattr(http_client, "DO_RATE_LIMIT", True) else None,
        max_per_second=1 if getattr(http_client, "DO_RATE_LIMIT", True) else None,
    ) as results:
        async for result in results:
            all_data.extend(result)

    all_data = sorted(all_data, key=operator.itemgetter("start_ts"))
    new_times = pd.date_range(start_ts, end_ts + pd.Timedelta(hours=1), freq=pd.Timedelta(minutes=30), inclusive="left")
    interpolated_data = interpolate_carbon_intensity(new_times, all_data)

    # We do this two step-copy to make sure that there are no clashes if we've requested overlapping periods of data.
    async with pool.acquire() as conn, conn.transaction():
        temp_table_suffix = int(abs(hash(datetime.datetime.now(datetime.UTC))))
        temp_table_name = f"grid_co2_{temp_table_suffix}"
        await conn.execute(f"CREATE TEMPORARY TABLE {temp_table_name} (LIKE carbon_intensity.grid_co2_cache)")
        await conn.copy_records_to_table(
            table_name=temp_table_name,
            columns=[
                "gsp_code",
                "start_ts",
                "end_ts",
                "forecast",
                "actual",
                "gas",
                "coal",
                "biomass",
                "nuclear",
                "hydro",
                "imports",
                "other",
                "wind",
                "solar",
            ],
            records=[
                (
                    gsp_code,
                    item["start_ts"],
                    item["end_ts"],
                    item.get("forecast"),
                    item.get("actual"),
                    item.get("gas"),
                    item.get("coal"),
                    item.get("biomass"),
                    item.get("nuclear"),
                    item.get("hydro"),
                    item.get("imports"),
                    item.get("other"),
                    item.get("wind"),
                    item.get("solar"),
                )
                for item in interpolated_data
            ],
        )
        await conn.execute(f"""
                INSERT INTO carbon_intensity.grid_co2_cache
                (SELECT
                    t.*
                FROM {temp_table_name} AS t)
                ON CONFLICT (gsp_code, start_ts) DO NOTHING""")
        await conn.execute(f"DROP TABLE {temp_table_name}")

    # Finally we're done, so return the good data that we've now filled in.
    rows = await pool.fetch(
        """
        SELECT
            start_ts,
            end_ts,
            actual,
            forecast
        FROM carbon_intensity.grid_co2_cache
        WHERE
            start_ts >= $1 AND end_ts <= $2 AND gsp_code = $3
        ORDER BY start_ts ASC""",
        start_ts,
        end_ts,
        gsp_code,
    )
    return [
        CarbonIntensityRawEntry(
            start_ts=item["start_ts"], end_ts=item["end_ts"], forecast=item["forecast"], actual=item["actual"]
        )
        for item in rows
    ]


@router.post("/generate-grid-co2", tags=["co2", "generate"])
async def generate_grid_co2(
    params: GridCO2Request, pool: DatabasePoolDep, http_client: HttpClientDep
) -> CarbonIntensityMetadata:
    """
    Get a grid CO2 carbon intensity from the National Grid API.

    If this is for a site where we have a stored postcode, it'll look up
    the regional generation mix for that specific region. If not, we'll just use the national mix.

    For the regional mix, we will get a `forecast` carbon itensity and maybe a regional breakdown into
    fuel sources. For the national mix, we'll get a `forecast` and an `actual`.

    This can be slow -- the rate limiting for the API is aggressive, so it
    takes approximately 1 second per 2 weeks of data.
    There is underlying caching which will should make repeated requests faster.

    Parameters
    ----------
    *params*
        A JSON body containing `{"site_id":..., "start_ts":..., "end_ts":...}
    pool
        Shared database connection pool to store the data in
    http_client
        Client with connection pool to make HTTP requests to CarbonIntensity API.

    Returns
    -------
    *metadata*
        Metadata about the grid CO2 information we've just put into the database.
    """
    try:
        postcode = await get_postcode(site_id=params.site_id, pool=pool)
    except ValueError:
        postcode = None
        logger.warning(f"No postcode found for {params.site_id}, using National data.")

    all_data = await fetch_carbon_intensity(
        http_client=http_client, postcode=postcode, start_ts=params.start_ts, end_ts=params.end_ts, pool=pool
    )

    metadata = CarbonIntensityMetadata(
        dataset_id=params.bundle_metadata.dataset_id if params.bundle_metadata is not None else uuid7(),
        created_at=datetime.datetime.now(datetime.UTC),
        data_source="api.carbonintensity.org.uk",
        is_regional=(postcode is not None),
        site_id=params.site_id,
    )

    async with pool.acquire() as conn, conn.transaction():
        await conn.execute(
            """
                INSERT INTO
                    carbon_intensity.metadata (
                        dataset_id,
                        created_at,
                        data_source,
                        is_regional,
                        site_id)
                VALUES ($1, $2, $3, $4, $5)""",
            metadata.dataset_id,
            metadata.created_at,
            metadata.data_source,
            metadata.is_regional,
            metadata.site_id,
        )

        await conn.copy_records_to_table(
            schema_name="carbon_intensity",
            table_name="grid_co2",
            columns=[
                "dataset_id",
                "start_ts",
                "end_ts",
                "forecast",
                "actual",
                "gas",
                "coal",
                "biomass",
                "nuclear",
                "hydro",
                "imports",
                "other",
                "wind",
                "solar",
            ],
            records=zip(
                itertools.repeat(metadata.dataset_id, len(all_data)),
                [item["start_ts"] for item in all_data],
                [item["end_ts"] for item in all_data],
                [item.get("forecast") for item in all_data],
                [item.get("actual") for item in all_data],
                [item.get("gas") for item in all_data],
                [item.get("coal") for item in all_data],
                [item.get("biomass") for item in all_data],
                [item.get("nuclear") for item in all_data],
                [item.get("hydro") for item in all_data],
                [item.get("imports") for item in all_data],
                [item.get("other") for item in all_data],
                [item.get("wind") for item in all_data],
                [item.get("solar") for item in all_data],
                strict=True,
            ),
        )

        if params.bundle_metadata is not None:
            assert params.bundle_metadata.dataset_type == DatasetTypeEnum.CarbonIntensity
            await file_self_with_bundle(pool=pool, bundle_metadata=params.bundle_metadata)

    logger.info(f"Grid CO2 generation {metadata.dataset_id} completed.")
    return metadata


@router.post("/get-grid-co2", tags=["co2", "get"])
async def get_grid_co2(params: DatasetIDWithTime, pool: DatabasePoolDep) -> EpochCarbonEntry:
    """
    Get a specific grid carbon itensity dataset that we generated with `generate-grid-co2`.

    The carbon itensity is measured in gCO2e / kWh. We get a forecast carbon intensity if
    this dataset was originally regional, and an actual carbon itensity if it was national.

    For regional carbon itensity readings, we also get a breakdown by generation source.
    The columns `["gas", "coal", "biomass", "nuclear", "hydro", "imports", "wind", "solar", "other"]`
    represent the fraction of total generation that came from that specific source (as calculated by
    National Grid).

    Parameters
    ----------
    params
        Database ID for a specific grid CO2 set, and the timestamps you're interested in.

    Returns
    -------
    carbon_intensity_entriesn
        A list of JSONed carbon intensity entries, maybe forecast or maybe actual.
    """
    res = await pool.fetch(
        """
        SELECT
            start_ts, forecast, actual
        FROM carbon_intensity.grid_co2
        WHERE dataset_id = $1
        AND $2 <= start_ts
        AND end_ts <= $3
        ORDER BY start_ts ASC""",
        params.dataset_id,
        params.start_ts,
        params.end_ts,
    )
    if res is None:
        raise HTTPException(
            400,
            f"Could not get a CarbonIntensity dataset for {params.dataset_id} between {params.start_ts} and {params.end_ts}",
        )
    carbon_df = pd.DataFrame.from_records(
        cast(RecordMapping, res), columns=["start_ts", "forecast", "actual"], coerce_float=True
    )
    carbon_df.index = pd.to_datetime(carbon_df["start_ts"])
    carbon_df["GridCO2"] = carbon_df["actual"].astype(float).fillna(carbon_df["forecast"].astype(float))
    carbon_df["GridCO2"] = carbon_df["GridCO2"].interpolate(method="time")
    return EpochCarbonEntry(timestamps=carbon_df["start_ts"].tolist(), data=carbon_df["GridCO2"].to_list())
