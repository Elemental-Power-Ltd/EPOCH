"""
Carbon Itensity endpoints, including generation and getting.

Carbon intensity is a measure of kg CO2 e / kWh for electrical power used, and
varies over time as the grid changes.
"""

import datetime
import itertools
import logging
import operator
import typing

import aiometer
import numpy as np
import pandas as pd
import pydantic
from fastapi import APIRouter, HTTPException

from ..dependencies import DatabasePoolDep, HTTPClient, HttpClientDep
from ..internal.client_data import get_postcode
from ..internal.site_manager.bundles import file_self_with_bundle
from ..internal.utils import chunk_time_period
from ..internal.utils.uuid import uuid7
from ..models.carbon_intensity import CarbonIntensityMetadata, EpochCarbonEntry, GridCO2Request
from ..models.core import DatasetIDWithTime

logger = logging.getLogger(__name__)
router = APIRouter()


class CarbonIntensityRawEntry(typing.TypedDict):
    """Data we get back from the CarbonIntensity API, including fuel types."""

    start_ts: pydantic.AwareDatetime
    end_ts: pydantic.AwareDatetime
    forecast: typing.NotRequired[float]
    actual: typing.NotRequired[float]
    biomass: typing.NotRequired[float]
    coal: typing.NotRequired[float]
    imports: typing.NotRequired[float]
    gas: typing.NotRequired[float]
    hydro: typing.NotRequired[float]
    nuclear: typing.NotRequired[float]
    other: typing.NotRequired[float]
    solar: typing.NotRequired[float]
    wind: typing.NotRequired[float]


def interpolate_carbon_intensity(
    new_times: pd.DatetimeIndex, raw_data: list[CarbonIntensityRawEntry]
) -> list[CarbonIntensityRawEntry]:
    """
    Interpolate a carbon intensity response to fill in the gaps.

    Often, the 3rd party API will have missing sections of data, either due to small bugs or totally missing readings.
    In those cases, we have to re-interpolate ourselves to fill those in, which isn't strictly accurate but
    is better than nothing (the fuel type interpolation will be especially inaccurate, as it may predict solar overnight).

    There are many different fuel types we'll have to interpolate over, which may and or may not be provided from the 3rd party.

    Parameters
    ----------
    new_times
        Series of datetimes to interpolate to, probably generated by pd.date_range.
        These will become start_ts values, and the end_ts values will be +30 minutes from these.

    raw_data
        List of raw data we got from the carbon intensity API, ideally with fuel types but maybe with chunks missing.

    Returns
    -------
        Data of same structure as raw_data, but interpolated linearly at the new_times
    """
    # Only the start_ts and end_ts are required keys, as the fuels and actual / forecast may not be present.
    keys = list(CarbonIntensityRawEntry.__optional_keys__)
    interpolated_data = {}

    # The x coordinates are differences from the specified start time, which can be negative
    # for numerical stability reasons (it's much easier to interpolate around 0 than it is to in
    # interpolate around 1e9).
    start_ts = min(new_times)
    xs = (new_times - start_ts).total_seconds().to_numpy()
    for key in keys:
        # We re-do the xs each time as we have to check if the relevant key is in this entry,
        # which is might not be.
        xp = np.asarray([
            (item["start_ts"] - start_ts).total_seconds()
            for item in raw_data
            if item.get(key) is not None and np.isfinite(item[key])  # type: ignore
        ])
        yp = np.asarray([
            item[key]  # type: ignore
            for item in raw_data
            if item.get(key) is not None and np.isfinite(item[key])  # type: ignore
        ])
        if len(xp) == 0 or len(yp) == 0:
            # We've got nothing here!
            interpolated_vals = np.full_like(xp, np.nan)
        else:
            interpolated_vals = np.interp(xs, xp, yp)
        interpolated_data[key] = interpolated_vals

    return [
        {
            "start_ts": time,
            "end_ts": time + pd.Timedelta(minutes=30),
            **{key: interpolated_data[key][i] for key in keys if len(interpolated_data[key]) > i},  # type: ignore
        }
        for i, time in enumerate(new_times)
    ]


async def fetch_carbon_intensity_batch(
    client: HTTPClient,
    postcode: str | None,
    timestamps: tuple[pydantic.AwareDatetime, pydantic.AwareDatetime],
) -> list[CarbonIntensityRawEntry]:
    """
    Fetch a single lot of data from the carbon itensity API.

    This is in its own (rather ugly) function to allow the rate limiter to function.
    Will return a `forecast` carbon itensity and a breakdown if using regional, or an actual if not.

    Parameters
    ----------
    client
        HTTPX AsyncClient that we re-use between connections

    postcode
        The entire postcode for this site with inbound and outbound codes split by a space (e.g. `SW1 1AA`).
        If None, we'll retrieve national data instead of regional.

    timestamps
        A (start_ts, end_ts) pairing for the time period we want to check. Should be within 14 days of each other.
        If these are the same, we'll push the end timestamp to be +24 hours, so watch out for getting extra data.

    Returns
    -------
    results
        A list of carbon intensity readings and their times. There is no guarantee that there are in order,
        and they may request data outside of the specified timestamp window.
    """
    fetch_start_ts, fetch_end_ts = timestamps
    if isinstance(fetch_start_ts, pd.Timestamp):
        fetch_start_ts = fetch_start_ts.to_pydatetime()
    if isinstance(fetch_end_ts, pd.Timestamp):
        fetch_end_ts = fetch_end_ts.to_pydatetime()

    # If we got the start and end the same, then this is probably trying to work around a bug
    # in the CarbonIntensity API which will stop at 23:30 sometimes.
    # In that case, fetch an extra day and let the next function sort it out.
    if fetch_start_ts == fetch_end_ts:
        fetch_end_ts = fetch_start_ts + pd.Timedelta(days=1)
    start_ts_str = fetch_start_ts.isoformat()
    end_ts_str = fetch_end_ts.isoformat()

    if postcode is not None:
        postcode_out = postcode.strip().split(" ")[0]
        ci_url = f"https://api.carbonintensity.org.uk/regional/intensity/{start_ts_str}/{end_ts_str}/postcode/{postcode_out}"
    else:
        # Retrieve national data instead of regional, as we didn't get a postcode.
        ci_url = f"https://api.carbonintensity.org.uk/intensity/{start_ts_str}/{end_ts_str}"

    response = await client.get(ci_url)
    if not response.status_code == 200:
        raise HTTPException(400, f"{ci_url} returned `{response.text}`")

    data = response.json()
    results: list[CarbonIntensityRawEntry] = []
    subdata = data["data"]
    if "data" in subdata:
        # Sometimes we get a nested object one deep, especially for regional data
        subdata = subdata["data"]

    for item in subdata:
        entry: CarbonIntensityRawEntry = {
            "start_ts": pd.to_datetime(item["from"]),
            "end_ts": pd.to_datetime(item["to"]),
            "forecast": item["intensity"].get("forecast", float("NaN")),
            "actual": item["intensity"].get("actual", float("NaN")),
        }
        for fuel_data in item.get("generationmix", []):
            entry[fuel_data["fuel"]] = fuel_data["perc"] / 100.0  # type: ignore
        results.append(entry)
    return results


async def fetch_carbon_intensity(
    client: HTTPClient,
    postcode: str | None,
    start_ts: pydantic.AwareDatetime,
    end_ts: pydantic.AwareDatetime,
) -> list[CarbonIntensityRawEntry]:
    """
    Fetch data from the carbon intensity API, doing the cleaning and interpolation as necessary.

    Will return a `forecast` carbon itensity and a breakdown if using regional, or an actual if not.
    Individual fuels are interpolated where possible, which may not be accurate. There's no guarantee that
    the fuel breakdown is correct, or provided by the 3rd party.

    Parameters
    ----------
    client
        HTTPX AsyncClient to make requests to the CarbonIntensity API with.

    postcode
        The entire postcode for this site with inbound and outbound codes split by a space (e.g. `SW1 1AA`).
        If provided, we'll request data for this region. If not, we'll request national data.

    start_ts
        The earliest timestamp to request data for such that min(data["start_ts"]) == start_ts

    end_ts
        The latest timestamp to request data for, such that max(data["end_ts"]) == end_ts
        We'll interpolate between these times if there's data missing on the service, and chunk the requests
        to be in ~14 day periods and split across years (to avoid a bug in the 3rd party)

    Returns
    -------
    results
        A list of carbon intensity readings and their times. These have been sorted and interpolated between
        the times.
    """
    # There's a bug in the CarbonIntensity API that doesn't like non-zero seconds, especially not closely spaced ones.
    # Instead, let's just grab the period 1 day either side and sort it out in the interpolation.
    rounded_start_ts = pd.Timestamp(start_ts).floor("1h").to_pydatetime()
    rounded_end_ts = pd.Timestamp(end_ts).ceil("1h").to_pydatetime()
    # There's another bug in the CarbonIntensity API that doesn't wrap around years. Instead,
    # we have to manually split the years here.
    time_pairs = chunk_time_period(rounded_start_ts, rounded_end_ts, pd.Timedelta(days=13), split_years=True)
    all_data: list[CarbonIntensityRawEntry] = []
    async with aiometer.amap(
        lambda ts_pair: fetch_carbon_intensity_batch(client=client, postcode=postcode, timestamps=ts_pair),
        time_pairs,
        # This is a horrible bodge, but for testing we have a mocked client
        # where we want to do
        max_at_once=1 if getattr(client, "DO_RATE_LIMIT", True) else None,
        max_per_second=1 if getattr(client, "DO_RATE_LIMIT", True) else None,
    ) as results:
        async for result in results:
            all_data.extend(result)

    if not all_data:
        raise HTTPException(400, "Failed to get grid CO2 data.")

    # Now we've got the data, we should tidy it up.
    # This involves sorting, filter and interpolate it
    all_data = sorted(all_data, key=operator.itemgetter("start_ts"))
    new_times = pd.date_range(start_ts, end_ts, freq=pd.Timedelta(minutes=30), inclusive="left")
    all_data = interpolate_carbon_intensity(new_times, all_data)
    return all_data


@router.post("/generate-grid-co2", tags=["co2", "generate"])
async def generate_grid_co2(
    params: GridCO2Request, pool: DatabasePoolDep, http_client: HttpClientDep
) -> CarbonIntensityMetadata:
    """
    Get a grid CO2 carbon intensity from the National Grid API.

    If this is for a site where we have a stored postcode, it'll look up
    the regional generation mix for that specific region. If not, we'll just use the national mix.

    For the regional mix, we will get a `forecast` carbon itensity and maybe a regional breakdown into
    fuel sources. For the national mix, we'll get a `forecast` and an `actual`.

    This can be slow -- the rate limiting for the API is aggressive, so it
    takes approximately 1 second per 2 weeks of data.

    Parameters
    ----------
    *params*
        A JSON body containing `{"site_id":..., "start_ts":..., "end_ts":...}
    pool
        Shared database connection pool to store the data in
    http_client
        Client with connection pool to make HTTP requests to CarbonIntensity API.

    Returns
    -------
    *metadata*
        Metadata about the grid CO2 information we've just put into the database.
    """
    try:
        postcode = await get_postcode(site_id=params.site_id, pool=pool)
    except ValueError:
        postcode = None
        logger.warning(f"No postcode found for {params.site_id}, using National data.")

    all_data = await fetch_carbon_intensity(
        client=http_client, postcode=postcode, start_ts=params.start_ts, end_ts=params.end_ts
    )

    metadata = CarbonIntensityMetadata(
        dataset_id=params.bundle_metadata.dataset_id if params.bundle_metadata is not None else uuid7(),
        created_at=datetime.datetime.now(datetime.UTC),
        data_source="api.carbonintensity.org.uk",
        is_regional=(postcode is not None),
        site_id=params.site_id,
    )

    async with pool.acquire() as conn:
        async with conn.transaction():
            await conn.execute(
                """
                INSERT INTO
                    carbon_intensity.metadata (
                        dataset_id,
                        created_at,
                        data_source,
                        is_regional,
                        site_id)
                VALUES ($1, $2, $3, $4, $5)""",
                metadata.dataset_id,
                metadata.created_at,
                metadata.data_source,
                metadata.is_regional,
                metadata.site_id,
            )

            await conn.copy_records_to_table(
                schema_name="carbon_intensity",
                table_name="grid_co2",
                columns=[
                    "dataset_id",
                    "start_ts",
                    "end_ts",
                    "forecast",
                    "actual",
                    "gas",
                    "coal",
                    "biomass",
                    "nuclear",
                    "hydro",
                    "imports",
                    "other",
                    "wind",
                    "solar",
                ],
                records=zip(
                    itertools.repeat(metadata.dataset_id, len(all_data)),
                    [item["start_ts"] for item in all_data],
                    [item["end_ts"] for item in all_data],
                    [item.get("forecast") for item in all_data],
                    [item.get("actual") for item in all_data],
                    [item.get("gas") for item in all_data],
                    [item.get("coal") for item in all_data],
                    [item.get("biomass") for item in all_data],
                    [item.get("nuclear") for item in all_data],
                    [item.get("hydro") for item in all_data],
                    [item.get("imports") for item in all_data],
                    [item.get("other") for item in all_data],
                    [item.get("wind") for item in all_data],
                    [item.get("solar") for item in all_data],
                    strict=True,
                ),
            )

            if params.bundle_metadata is not None:
                await file_self_with_bundle(pool=pool, bundle_metadata=params.bundle_metadata)

    logger.info(f"Grid CO2 generation {metadata.dataset_id} completed.")
    return metadata


@router.post("/get-grid-co2", tags=["co2", "get"])
async def get_grid_co2(params: DatasetIDWithTime, pool: DatabasePoolDep) -> EpochCarbonEntry:
    """
    Get a specific grid carbon itensity dataset that we generated with `generate-grid-co2`.

    The carbon itensity is measured in gCO2e / kWh. We get a forecast carbon intensity if
    this dataset was originally regional, and an actual carbon itensity if it was national.

    For regional carbon itensity readings, we also get a breakdown by generation source.
    The columns `["gas", "coal", "biomass", "nuclear", "hydro", "imports", "wind", "solar", "other"]`
    represent the fraction of total generation that came from that specific source (as calculated by
    National Grid).

    Parameters
    ----------
    params
        Database ID for a specific grid CO2 set, and the timestamps you're interested in.

    Returns
    -------
    carbon_intensity_entriesn
        A list of JSONed carbon intensity entries, maybe forecast or maybe actual.
    """
    res = await pool.fetch(
        """
        SELECT
            start_ts, forecast, actual
        FROM carbon_intensity.grid_co2
        WHERE dataset_id = $1
        AND $2 <= start_ts
        AND end_ts <= $3
        ORDER BY start_ts ASC""",
        params.dataset_id,
        params.start_ts,
        params.end_ts,
    )
    carbon_df = pd.DataFrame.from_records(res, columns=["start_ts", "forecast", "actual"], coerce_float=True)
    carbon_df.index = pd.to_datetime(carbon_df["start_ts"])  # type: ignore

    # TODO (2025-01-09 MHJB): fix this awful pandas repeated interpolation, reindexing and resampling, it's a mess
    # carbon_df = carbon_df.resample(pd.Timedelta(minutes=30)).max().infer_objects().interpolate(method="time")
    # carbon_df = carbon_df.reindex(
    #    pd.DatetimeIndex(pd.date_range(params.start_ts, params.end_ts, freq=pd.Timedelta(minutes=30), inclusive="left"))
    # )

    carbon_df["GridCO2"] = carbon_df["actual"].astype(float).fillna(carbon_df["forecast"].astype(float))
    carbon_df["GridCO2"] = carbon_df["GridCO2"].interpolate(method="time")
    return EpochCarbonEntry(timestamps=carbon_df["start_ts"].tolist(), data=carbon_df["GridCO2"].to_list())
